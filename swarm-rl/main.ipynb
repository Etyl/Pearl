{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:29.525795Z",
     "start_time": "2025-01-01T18:48:29.503975Z"
    }
   },
   "source": [
    "import json\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from pearl.replay_buffers import BasicReplayBuffer\n",
    "from pearl.utils.instantiations.spaces import DiscreteActionSpace\n",
    "from pearl import PearlAgent\n",
    "from pearl.utils.functional_utils.train_and_eval.learning_logger import LearningLogger, null_learning_logger\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import TRAINING_TAG\n",
    "from pearl.replay_buffers import TransitionBatch, ReplayBuffer\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.neural_networks.sequential_decision_making import VanillaContinuousActorNetwork\n",
    "from pearl.policy_learners.sequential_decision_making import ImplicitQLearning\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import offline_evaluation,offline_learning\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork, VanillaQValueMultiHeadNetwork\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "ad5932d4a9f11e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:29.539630Z",
     "start_time": "2025-01-01T18:48:29.525795Z"
    }
   },
   "source": [
    "FRONTIER_COUNT:int = 6 # the maximum number of selected frontiers\n",
    "FRONTIER_FEATURES:int = 6 # the number of features measured for each frontier\n",
    "OTHER_FRONTIER_INPUTS:int = 3 # other observations (current % explored area)\n",
    "\n",
    "OBSERVATION_SPACE:int = FRONTIER_COUNT*FRONTIER_FEATURES+OTHER_FRONTIER_INPUTS\n",
    "ACTION_SPACE:int = FRONTIER_COUNT"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "a3937b0139a19940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:29.557310Z",
     "start_time": "2025-01-01T18:48:29.551970Z"
    }
   },
   "source": [
    "replay_buffer_size = 1_000_000\n",
    "device = \"cpu\"\n",
    "is_action_continuous = False\n",
    "data_file_path = \"data/rl-run-data.json\"\n",
    "max_number_actions_if_discrete = ACTION_SPACE"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "e6f54912061804ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:46.449020Z",
     "start_time": "2025-01-01T18:48:29.570777Z"
    }
   },
   "source": [
    "with open(data_file_path, \"r\") as f:\n",
    "    data_transitions = json.load(f)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "801dfefd86e8147e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T19:00:39.947750Z",
     "start_time": "2025-01-01T18:58:14.908203Z"
    }
   },
   "source": [
    "\n",
    "offline_data_replay_buffer = BasicReplayBuffer(replay_buffer_size)\n",
    "if is_action_continuous:\n",
    "    offline_data_replay_buffer._is_action_continuous = True\n",
    "count = 0\n",
    "for map_name in data_transitions:\n",
    "    for i in range(len(data_transitions[map_name])):\n",
    "        transitions = data_transitions[map_name][i][\"run\"]\n",
    "        for j in range(len(transitions[\"obs\"])):\n",
    "            transition = {}\n",
    "            transition[\"observation\"] = transitions[\"obs\"][j]\n",
    "            transition[\"action\"] = np.argmax(transitions[\"action\"][j])\n",
    "            transition[\"next_observation\"] = transitions[\"next_obs\"][j]\n",
    "            transition[\"reward\"] = transitions[\"reward\"][j]\n",
    "            transition[\"curr_available_actions\"] = DiscreteActionSpace(\n",
    "                actions=list(\n",
    "                    torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                )\n",
    "            )\n",
    "            transition[\"next_available_actions\"] = DiscreteActionSpace(\n",
    "                actions=list(\n",
    "                    torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                )\n",
    "            )\n",
    "            transition[\"done\"] = False\n",
    "            \n",
    "            assert len(transitions[\"action\"][j]) == ACTION_SPACE\n",
    "            assert len(transition[\"observation\"]) == OBSERVATION_SPACE\n",
    "            assert len(transition[\"next_observation\"]) == OBSERVATION_SPACE\n",
    "            assert len(transition[\"reward\"]) == 1\n",
    "            \n",
    "            count += 1\n",
    "            offline_data_replay_buffer.push(\n",
    "                state=transition[\"observation\"],\n",
    "                action=transition[\"action\"],\n",
    "                reward=transition[\"reward\"],\n",
    "                next_state=transition[\"next_observation\"],\n",
    "                curr_available_actions=transition[\"curr_available_actions\"],\n",
    "                next_available_actions=transition[\"next_available_actions\"],\n",
    "                terminated=transition[\"done\"],\n",
    "                truncated=False,\n",
    "                max_number_actions=max_number_actions_if_discrete,\n",
    "            )\n",
    "print(f\"{count} transitions saved\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488915\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "324fa51831dfb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T19:01:10.524136Z",
     "start_time": "2025-01-01T19:01:10.461144Z"
    }
   },
   "source": [
    "Q_value_network = VanillaQValueMultiHeadNetwork(\n",
    "    state_dim=OBSERVATION_SPACE,  # dimension of the state representation\n",
    "    action_dim=ACTION_SPACE,                    # dimension of the action representation\n",
    "    hidden_dims=[1024, 1024],                   # dimensions of the intermediate layers\n",
    "    output_dim=ACTION_SPACE\n",
    ")    \n",
    "\n",
    "action_space = DiscreteActionSpace(\n",
    "    actions=list(torch.arange(ACTION_SPACE).view(-1, 1))\n",
    ")\n",
    "\n",
    "agent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=OBSERVATION_SPACE,\n",
    "        action_space=action_space,\n",
    "        batch_size=512,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_value_network, # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=ACTION_SPACE\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "c44aac56bc2d3f1d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-01T18:48:46.806952Z"
    }
   },
   "source": [
    "# Number of training epochs\n",
    "training_epochs = 50000\n",
    "experiment_seed = 100\n",
    "\n",
    "offline_learning(\n",
    "    offline_agent=agent,\n",
    "    data_buffer=offline_data_replay_buffer, # replay buffer created using the offline data\n",
    "    training_epochs=training_epochs,\n",
    "    seed=experiment_seed,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 647/117188 [00:09<28:24, 68.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m training_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50000\u001B[39m\n\u001B[0;32m      3\u001B[0m experiment_seed \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m----> 5\u001B[0m \u001B[43moffline_learning\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffline_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffline_data_replay_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# replay buffer created using the offline data\u001B[39;49;00m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_seed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\utils\\functional_utils\\train_and_eval\\offline_learning_and_evaluation.py:192\u001B[0m, in \u001B[0;36moffline_learning\u001B[1;34m(offline_agent, data_buffer, training_epochs, number_of_batches, learning_logger, seed)\u001B[0m\n\u001B[0;32m    190\u001B[0m batch \u001B[38;5;241m=\u001B[39m data_buffer\u001B[38;5;241m.\u001B[39msample(offline_agent\u001B[38;5;241m.\u001B[39mpolicy_learner\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch, TransitionBatch)\n\u001B[1;32m--> 192\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43moffline_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    194\u001B[0m     learning_logger(loss, i, batch, TRAINING_TAG)\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\pearl_agent.py:231\u001B[0m, in \u001B[0;36mPearlAgent.learn_batch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;124;03mThis API is often used in offline learning\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;124;03mwhere users pass in a batch of data to train directly\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    230\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy_learner\u001B[38;5;241m.\u001B[39mpreprocess_batch(batch)\n\u001B[1;32m--> 231\u001B[0m policy_learner_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msafety_module\u001B[38;5;241m.\u001B[39mlearn_batch(batch)\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m policy_learner_loss\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\policy_learners\\sequential_decision_making\\deep_td_learning.py:288\u001B[0m, in \u001B[0;36mDeepTDLearning.learn_batch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m reward_batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m batch_size\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m terminated_batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m batch_size\n\u001B[1;32m--> 288\u001B[0m state_action_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_Q\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_q_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstate_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43maction_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcurr_available_actions_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurr_available_actions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (batch_size)\u001B[39;00m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;66;03m# for duelling dqn, specifying the `curr_available_actions_batch` field takes care of\u001B[39;00m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;66;03m# the mean subtraction for advantage estimation\u001B[39;00m\n\u001B[0;32m    295\u001B[0m \n\u001B[0;32m    296\u001B[0m \u001B[38;5;66;03m# Compute the Bellman Target\u001B[39;00m\n\u001B[0;32m    297\u001B[0m expected_state_action_values \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    298\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_next_state_values(batch, batch_size)\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_discount_factor\n\u001B[0;32m    300\u001B[0m     \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m terminated_batch\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m    301\u001B[0m ) \u001B[38;5;241m+\u001B[39m reward_batch\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (batch_size), r + gamma * V(s)\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\neural_networks\\sequential_decision_making\\q_value_networks.py:234\u001B[0m, in \u001B[0;36mVanillaQValueMultiHeadNetwork.get_q_values\u001B[1;34m(self, state_batch, action_batch, curr_available_actions_batch)\u001B[0m\n\u001B[0;32m    228\u001B[0m     extended_action_batch \u001B[38;5;241m=\u001B[39m action_batch\n\u001B[0;32m    230\u001B[0m \u001B[38;5;66;03m# We obtain the values for all actions and filter the queries/available actions\u001B[39;00m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;66;03m# by multiplying q_values for all actions by the one-hot action batch.\u001B[39;00m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;66;03m# We unsqueeze the last dimension to make the q-value column vectors matrices\u001B[39;00m\n\u001B[0;32m    233\u001B[0m \u001B[38;5;66;03m# so they can be multiplied with torch.bmm. Afterwards we squeeze the added dimension back.\u001B[39;00m\n\u001B[1;32m--> 234\u001B[0m q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_batch\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munsqueeze(\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    236\u001B[0m )  \u001B[38;5;66;03m# (batch_size x num actions x 1)\u001B[39;00m\n\u001B[0;32m    237\u001B[0m q_values \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbmm(\n\u001B[0;32m    238\u001B[0m     extended_action_batch,  \u001B[38;5;66;03m# shape: (batch_size, number of query actions, num actions)\u001B[39;00m\n\u001B[0;32m    239\u001B[0m     q_values,  \u001B[38;5;66;03m# (batch_size x num actions x 1)\u001B[39;00m\n\u001B[0;32m    240\u001B[0m )  \u001B[38;5;66;03m# (batch_size x number of query actions x 1)\u001B[39;00m\n\u001B[0;32m    241\u001B[0m q_values \u001B[38;5;241m=\u001B[39m q_values\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (batch_size x number of query actions)\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\neural_networks\\sequential_decision_making\\q_value_networks.py:211\u001B[0m, in \u001B[0;36mVanillaQValueMultiHeadNetwork.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\.env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:56.254711500Z",
     "start_time": "2025-01-01T18:39:23.591386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"agent.pkl\", \"wb\") as f:\n",
    "    pickle.dump(agent, f)\n",
    "    \n",
    "with open(\"Q_func.pkl\", \"wb\") as f:\n",
    "    pickle.dump(agent.policy_learner._Q._model, f)"
   ],
   "id": "b4260a2ea2068b4",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:56.270331800Z",
     "start_time": "2025-01-01T18:20:30.787783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trans = offline_data_replay_buffer.sample(1)\n",
    "agent.reset(trans.state[0],action_space)\n",
    "agent.act(exploit=True)"
   ],
   "id": "8567b923ee978909",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:56.285952100Z",
     "start_time": "2025-01-01T18:37:37.582388Z"
    }
   },
   "cell_type": "code",
   "source": "agent.policy_learner._Q._model",
   "id": "f016041d6aba50c1",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241m.\u001B[39mpolicy_learner\u001B[38;5;241m.\u001B[39m_Q\u001B[38;5;241m.\u001B[39m_model\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional 2024.2.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1207\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1204\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1206\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1207\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional 2024.2.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1222\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1219\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1221\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1222\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1224\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1226\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f215c57ed4c2d4ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
