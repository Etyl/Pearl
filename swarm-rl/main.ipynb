{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-01-02T22:13:34.901243Z",
     "start_time": "2025-01-02T22:13:30.064683Z"
    }
   },
   "source": [
    "import json\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from pearl.replay_buffers import BasicReplayBuffer\n",
    "from pearl.utils.instantiations.spaces import DiscreteActionSpace\n",
    "from pearl import PearlAgent\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import offline_learning\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork, VanillaQValueMultiHeadNetwork\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ad5932d4a9f11e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T22:13:44.763502Z",
     "start_time": "2025-01-02T22:13:44.760030Z"
    }
   },
   "source": [
    "FRONTIER_COUNT:int = 6 # the maximum number of selected frontiers\n",
    "FRONTIER_FEATURES:int = 6 # the number of features measured for each frontier\n",
    "OTHER_FRONTIER_INPUTS:int = 3 # other observations (current % explored area)\n",
    "\n",
    "OBSERVATION_SPACE:int = FRONTIER_COUNT*FRONTIER_FEATURES+OTHER_FRONTIER_INPUTS\n",
    "ACTION_SPACE:int = FRONTIER_COUNT"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a3937b0139a19940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T22:13:45.828996Z",
     "start_time": "2025-01-02T22:13:45.819077Z"
    }
   },
   "source": [
    "replay_buffer_size = 1_000_000\n",
    "device = \"cpu\"\n",
    "data_file_paths = [\"data/rl-run-data.json\", \"data/rl-run-dqn-1-data.json\"]\n",
    "max_number_actions_if_discrete = ACTION_SPACE\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T22:31:44.778033Z",
     "start_time": "2025-01-02T22:31:44.769105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_transitions(buffer, path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data_transitions = json.load(f)\n",
    "    \n",
    "    count = 0\n",
    "    for map_name in data_transitions:\n",
    "        for i in range(len(data_transitions[map_name])):\n",
    "            transitions = data_transitions[map_name][i][\"run\"]\n",
    "            for j in range(len(transitions[\"obs\"])):\n",
    "                transition = {}\n",
    "                \n",
    "                transition[\"observation\"] = np.array(transitions[\"obs\"][j])\n",
    "                transition[\"action\"] = np.argmax(transitions[\"action\"][j])\n",
    "                transition[\"next_observation\"] = np.array(transitions[\"next_obs\"][j])\n",
    "                transition[\"reward\"] = transitions[\"reward\"][j]\n",
    "                transition[\"curr_available_actions\"] = DiscreteActionSpace(\n",
    "                    actions=list(\n",
    "                        torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                    )\n",
    "                )\n",
    "                transition[\"next_available_actions\"] = DiscreteActionSpace(\n",
    "                    actions=list(\n",
    "                        torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                    )\n",
    "                )\n",
    "                transition[\"done\"] = False\n",
    "                \n",
    "                # Ignore terminated transitions\n",
    "                if transition[\"reward\"][0] > 0.995:\n",
    "                    continue\n",
    "                    \n",
    "                # Suffle frontier order\n",
    "                permutation = np.random.permutation(FRONTIER_COUNT)\n",
    "                \n",
    "                for obs in [\"observation\", \"next_observation\"]:\n",
    "                    for i in range(FRONTIER_FEATURES):               \n",
    "                        transition[obs][FRONTIER_COUNT*i:FRONTIER_COUNT*(i+1)] = transition[obs][FRONTIER_COUNT*i:FRONTIER_COUNT*(i+1)][permutation]\n",
    "                \n",
    "                transition[\"action\"] = permutation[transition[\"action\"]]\n",
    "\n",
    "                assert len(transitions[\"action\"][j]) == ACTION_SPACE\n",
    "                assert len(transition[\"observation\"]) == OBSERVATION_SPACE\n",
    "                assert len(transition[\"next_observation\"]) == OBSERVATION_SPACE\n",
    "                assert len(transition[\"reward\"]) == 1\n",
    "                \n",
    "                count += 1\n",
    "                buffer.push(\n",
    "                    state=transition[\"observation\"],\n",
    "                    action=transition[\"action\"],\n",
    "                    reward=transition[\"reward\"],\n",
    "                    next_state=transition[\"next_observation\"],\n",
    "                    curr_available_actions=transition[\"curr_available_actions\"],\n",
    "                    next_available_actions=transition[\"next_available_actions\"],\n",
    "                    terminated=transition[\"done\"],\n",
    "                    truncated=False,\n",
    "                    max_number_actions=max_number_actions_if_discrete,\n",
    "                )\n",
    "    print(f\"Loaded {count} transitions from {path}\")\n"
   ],
   "id": "801dfefd86e8147e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T22:32:26.044387700Z",
     "start_time": "2025-01-02T22:31:47.661383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "offline_data_replay_buffer = BasicReplayBuffer(replay_buffer_size)\n",
    "\n",
    "for data_path in data_file_paths:\n",
    "    load_transitions(offline_data_replay_buffer, data_path)"
   ],
   "id": "f416afb7b6269634",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m offline_data_replay_buffer \u001B[38;5;241m=\u001B[39m BasicReplayBuffer(replay_buffer_size)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data_path \u001B[38;5;129;01min\u001B[39;00m data_file_paths:\n\u001B[1;32m----> 4\u001B[0m     \u001B[43mload_transitions\u001B[49m\u001B[43m(\u001B[49m\u001B[43moffline_data_replay_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[12], line 47\u001B[0m, in \u001B[0;36mload_transitions\u001B[1;34m(buffer, path)\u001B[0m\n\u001B[0;32m     44\u001B[0m             \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(transition[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     46\u001B[0m             count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 47\u001B[0m             \u001B[43mbuffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpush\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mobservation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m                \u001B[49m\u001B[43maction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m                \u001B[49m\u001B[43mreward\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreward\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m                \u001B[49m\u001B[43mnext_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnext_observation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m                \u001B[49m\u001B[43mcurr_available_actions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcurr_available_actions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m                \u001B[49m\u001B[43mnext_available_actions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnext_available_actions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m                \u001B[49m\u001B[43mterminated\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdone\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                \u001B[49m\u001B[43mtruncated\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmax_number_actions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_number_actions_if_discrete\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoaded \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m transitions from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\replay_buffers\\tensor_based_replay_buffer.py:124\u001B[0m, in \u001B[0;36mTensorBasedReplayBuffer.push\u001B[1;34m(self, state, action, reward, terminated, truncated, curr_available_actions, next_state, next_available_actions, max_number_actions, cost)\u001B[0m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m next_unavailable_actions_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m         next_unavailable_actions_mask \u001B[38;5;241m=\u001B[39m next_unavailable_actions_mask\u001B[38;5;241m.\u001B[39munsqueeze(\n\u001B[0;32m    121\u001B[0m             \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    122\u001B[0m         )\n\u001B[1;32m--> 124\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_store_transition\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreward\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43mterminated\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtruncated\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcurr_available_actions_tensor_with_padding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcurr_unavailable_actions_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_available_actions_tensor_with_padding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnext_unavailable_actions_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcost\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\replay_buffers\\basic_replay_buffer.py:40\u001B[0m, in \u001B[0;36mBasicReplayBuffer._store_transition\u001B[1;34m(self, state, action, reward, terminated, truncated, curr_available_actions_tensor_with_padding, curr_unavailable_actions_mask, next_state, next_available_actions_tensor_with_padding, next_unavailable_actions_mask, cost)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_store_transition\u001B[39m(\n\u001B[0;32m     23\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m     24\u001B[0m     state: SubjectiveState,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     34\u001B[0m     cost: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     36\u001B[0m     transition \u001B[38;5;241m=\u001B[39m Transition(\n\u001B[0;32m     37\u001B[0m         state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_non_optional_single_state(state),\n\u001B[0;32m     38\u001B[0m         action\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_action(action),\n\u001B[0;32m     39\u001B[0m         reward\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_reward(reward),\n\u001B[1;32m---> 40\u001B[0m         next_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_single_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     41\u001B[0m         curr_available_actions\u001B[38;5;241m=\u001B[39mcurr_available_actions_tensor_with_padding,\n\u001B[0;32m     42\u001B[0m         curr_unavailable_actions_mask\u001B[38;5;241m=\u001B[39mcurr_unavailable_actions_mask,\n\u001B[0;32m     43\u001B[0m         next_available_actions\u001B[38;5;241m=\u001B[39mnext_available_actions_tensor_with_padding,\n\u001B[0;32m     44\u001B[0m         next_unavailable_actions_mask\u001B[38;5;241m=\u001B[39mnext_unavailable_actions_mask,\n\u001B[0;32m     45\u001B[0m         terminated\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_terminated(terminated),\n\u001B[0;32m     46\u001B[0m         truncated\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_truncated(truncated),\n\u001B[0;32m     47\u001B[0m         cost\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_single_cost(cost),\n\u001B[0;32m     48\u001B[0m     )\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mappend(transition)\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\replay_buffers\\tensor_based_replay_buffer.py:152\u001B[0m, in \u001B[0;36mTensorBasedReplayBuffer._process_single_state\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_non_optional_single_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\replay_buffers\\tensor_based_replay_buffer.py:160\u001B[0m, in \u001B[0;36mTensorBasedReplayBuffer._process_non_optional_single_state\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m state\u001B[38;5;241m.\u001B[39mto(get_default_device())\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "324fa51831dfb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:47:29.031112Z",
     "start_time": "2025-01-02T18:47:28.991929Z"
    }
   },
   "source": [
    "Q_value_network = VanillaQValueMultiHeadNetwork(\n",
    "    state_dim=OBSERVATION_SPACE,  # dimension of the state representation\n",
    "    action_dim=ACTION_SPACE,                    # dimension of the action representation\n",
    "    hidden_dims=[1024, 1024],                   # dimensions of the intermediate layers\n",
    "    output_dim=ACTION_SPACE\n",
    ")    \n",
    "\n",
    "action_space = DiscreteActionSpace(\n",
    "    actions=list(torch.arange(ACTION_SPACE).view(-1, 1))\n",
    ")\n",
    "\n",
    "agent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=OBSERVATION_SPACE,\n",
    "        action_space=action_space,\n",
    "        batch_size=512,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_value_network, # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=ACTION_SPACE\n",
    "        ),\n",
    "        is_conservative=True\n",
    "    )\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "c44aac56bc2d3f1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:47:40.150528200Z",
     "start_time": "2025-01-02T18:47:33.714175Z"
    }
   },
   "source": [
    "# Number of training epochs\n",
    "training_epochs = 100\n",
    "experiment_seed = 100\n",
    "\n",
    "offline_learning(\n",
    "    offline_agent=agent,\n",
    "    data_buffer=offline_data_replay_buffer, # replay buffer created using the offline data\n",
    "    training_epochs=training_epochs,\n",
    "    seed=experiment_seed,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/114125 [00:06<16:55:51,  1.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m training_epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m      3\u001B[0m experiment_seed \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m----> 5\u001B[0m \u001B[43moffline_learning\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffline_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffline_data_replay_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# replay buffer created using the offline data\u001B[39;49;00m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_seed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\utils\\functional_utils\\train_and_eval\\offline_learning_and_evaluation.py:190\u001B[0m, in \u001B[0;36moffline_learning\u001B[1;34m(offline_agent, data_buffer, training_epochs, number_of_batches, learning_logger, seed)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;66;03m# training loop\u001B[39;00m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(number_of_batches)):\n\u001B[1;32m--> 190\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mdata_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43moffline_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_learner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(batch, TransitionBatch)\n\u001B[0;32m    192\u001B[0m     loss \u001B[38;5;241m=\u001B[39m offline_agent\u001B[38;5;241m.\u001B[39mlearn_batch(batch\u001B[38;5;241m=\u001B[39mbatch)\n",
      "File \u001B[1;32m~\\Documents\\Telecom\\Pearl\\pearl\\replay_buffers\\tensor_based_replay_buffer.py:279\u001B[0m, in \u001B[0;36mTensorBasedReplayBuffer.sample\u001B[1;34m(self, batch_size)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    276\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt get a batch of size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from a replay buffer with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    277\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m elements\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    278\u001B[0m     )\n\u001B[1;32m--> 279\u001B[0m samples \u001B[38;5;241m=\u001B[39m \u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_transition_batch(\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;66;03m# pyre-fixme[6]: For 1st argument expected `List[Transition]` but got\u001B[39;00m\n\u001B[0;32m    282\u001B[0m     \u001B[38;5;66;03m#  `List[Union[Transition, TransitionBatch]]`.\u001B[39;00m\n\u001B[0;32m    283\u001B[0m     transitions\u001B[38;5;241m=\u001B[39msamples,\n\u001B[0;32m    284\u001B[0m     is_action_continuous\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_action_continuous,\n\u001B[0;32m    285\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\random.py:473\u001B[0m, in \u001B[0;36mRandom.sample\u001B[1;34m(self, population, k, counts)\u001B[0m\n\u001B[0;32m    471\u001B[0m selected_add \u001B[38;5;241m=\u001B[39m selected\u001B[38;5;241m.\u001B[39madd\n\u001B[0;32m    472\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(k):\n\u001B[1;32m--> 473\u001B[0m     j \u001B[38;5;241m=\u001B[39m \u001B[43mrandbelow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m selected:\n\u001B[0;32m    475\u001B[0m         j \u001B[38;5;241m=\u001B[39m randbelow(n)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\random.py:235\u001B[0m, in \u001B[0;36mRandom._randbelow_with_getrandbits\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m    232\u001B[0m             \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_randbelow \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_randbelow_without_getrandbits\n\u001B[0;32m    233\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m--> 235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_randbelow_with_getrandbits\u001B[39m(\u001B[38;5;28mself\u001B[39m, n):\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReturn a random int in the range [0,n).  Defined for n > 0.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    238\u001B[0m     getrandbits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetrandbits\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:56.254711500Z",
     "start_time": "2025-01-01T18:39:23.591386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"agent.pkl\", \"wb\") as f:\n",
    "    pickle.dump(agent, f)\n",
    "    \n",
    "torch.save(agent.policy_learner._Q._model, \"Q_function.pth\")"
   ],
   "id": "b4260a2ea2068b4",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-01T18:48:56.270331800Z",
     "start_time": "2025-01-01T18:20:30.787783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trans = offline_data_replay_buffer.sample(1)\n",
    "agent.reset(trans.state[0],action_space)\n",
    "agent.act(exploit=True)"
   ],
   "id": "8567b923ee978909",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T17:58:45.135148Z",
     "start_time": "2025-01-02T17:58:45.081580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"agent.pkl\", \"rb\") as f:\n",
    "    agent = pickle.load(f)"
   ],
   "id": "f016041d6aba50c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T17:59:39.404979Z",
     "start_time": "2025-01-02T17:59:39.393572Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(agent.policy_learner._Q._model, \"Q_function.pth\")",
   "id": "f215c57ed4c2d4ec",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
