{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:30:32.786245Z",
     "start_time": "2025-01-06T00:30:32.774170Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from typing import Optional, Any\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from custom_buffer import WeightedReplayBuffer\n",
    "\n",
    "from pearl.replay_buffers import BasicReplayBuffer\n",
    "from pearl.utils.instantiations.spaces import DiscreteActionSpace\n",
    "from pearl import PearlAgent\n",
    "from pearl.utils.functional_utils.train_and_eval.learning_logger import LearningLogger, null_learning_logger\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import TRAINING_TAG\n",
    "from pearl.replay_buffers import TransitionBatch, ReplayBuffer\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.neural_networks.sequential_decision_making import VanillaContinuousActorNetwork\n",
    "from pearl.policy_learners.sequential_decision_making import ImplicitQLearning\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import offline_evaluation,offline_learning\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork, VanillaQValueMultiHeadNetwork\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import MultiAgentQValueMultiHeadNetwork\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5932d4a9f11e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:30:32.797335Z",
     "start_time": "2025-01-06T00:30:32.792288Z"
    }
   },
   "outputs": [],
   "source": [
    "FRONTIER_COUNT:int = 6 # the maximum number of selected frontiers\n",
    "FRONTIER_FEATURES:int = 6 # the number of features measured for each frontier\n",
    "OTHER_FRONTIER_INPUTS:int = 3 # other observations (current % explored area)\n",
    "\n",
    "OBSERVATION_SPACE:int = FRONTIER_COUNT*FRONTIER_FEATURES+OTHER_FRONTIER_INPUTS\n",
    "ACTION_SPACE:int = FRONTIER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3216b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:30:32.825698Z",
     "start_time": "2025-01-06T00:30:32.814261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 3\n",
      "GPU 0: Tesla V100-PCIE-16GB\n",
      "GPU 1: Tesla V100-PCIE-32GB\n",
      "GPU 2: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3937b0139a19940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:30:32.867684Z",
     "start_time": "2025-01-06T00:30:32.857034Z"
    }
   },
   "outputs": [],
   "source": [
    "replay_buffer_size = 1_000_000\n",
    "device_id = 0\n",
    "is_action_continuous = False\n",
    "# data_file_paths = [\n",
    "#     \"data/rl-run-data.json\", \n",
    "#     \"data/rl-run-dqn-1-data.json\",\n",
    "#     \"data/rl-run-dqn-7-data.json\"]\n",
    "data_file_paths = [\"data/rl-run-data.json\"]\n",
    "max_number_actions_if_discrete = ACTION_SPACE\n",
    "loss_log_file = \"loss.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801dfefd86e8147e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:30:32.889773Z",
     "start_time": "2025-01-06T00:30:32.875776Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_transitions(buffer, path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data_transitions = json.load(f)\n",
    "    \n",
    "    count = 0\n",
    "    for map_name in data_transitions:\n",
    "        for i in range(len(data_transitions[map_name])):\n",
    "            transitions = data_transitions[map_name][i][\"run\"]\n",
    "            episode = defaultdict(list)\n",
    "            for j in range(len(transitions[\"obs\"])):\n",
    "                transition = {}\n",
    "                \n",
    "                transition[\"observation\"] = np.array(transitions[\"obs\"][j])\n",
    "                transition[\"action\"] = np.argmax(transitions[\"action\"][j])\n",
    "                transition[\"next_observation\"] = np.array(transitions[\"next_obs\"][j])\n",
    "                transition[\"reward\"] = (\n",
    "                    transitions[\"reward\"][j][0] +\n",
    "                    10*transitions[\"obs\"][j][37]*transitions[\"obs\"][j][38]\n",
    "                )                \n",
    "                transition[\"curr_available_actions\"] = DiscreteActionSpace(\n",
    "                    actions=list(\n",
    "                        torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                    )\n",
    "                )\n",
    "                transition[\"next_available_actions\"] = DiscreteActionSpace(\n",
    "                    actions=list(\n",
    "                        torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                    )\n",
    "                )\n",
    "                transition[\"terminated\"] = False\n",
    "                transition[\"truncated\"] = False\n",
    "                \n",
    "                # Ignore terminated transitions\n",
    "                if transition[\"reward\"] > 0.995:\n",
    "                    continue\n",
    "                    \n",
    "                # Suffle frontier order\n",
    "                permutation = np.random.permutation(FRONTIER_COUNT)\n",
    "                \n",
    "                for obs in [\"observation\", \"next_observation\"]:\n",
    "                    for i in range(FRONTIER_FEATURES):               \n",
    "                        transition[obs][FRONTIER_COUNT*i:FRONTIER_COUNT*(i+1)] = transition[obs][FRONTIER_COUNT*i:FRONTIER_COUNT*(i+1)][permutation]\n",
    "                \n",
    "                transition[\"action\"] = permutation[transition[\"action\"]]\n",
    "\n",
    "                assert len(transitions[\"action\"][j]) == ACTION_SPACE\n",
    "                assert len(transition[\"observation\"]) == OBSERVATION_SPACE\n",
    "                assert len(transition[\"next_observation\"]) == OBSERVATION_SPACE\n",
    "                \n",
    "                count += 1\n",
    "                for key in transition:\n",
    "                    episode[key].append(transition[key])\n",
    "                    \n",
    "            buffer.add_traj(episode, map_name)\n",
    "    print(f\"Loaded {count} transitions from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b8eba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:32:03.005300Z",
     "start_time": "2025-01-06T00:30:32.904194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in buffer...\n",
      "Loaded 206585 transitions from data/rl-run-data.json\n",
      "Building buffer...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'DiscreteActionSpace'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     load_transitions(offline_data_replay_buffer, data_path)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding buffer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43moffline_data_replay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Pearl/swarm-rl/custom_buffer.py:144\u001b[0m, in \u001b[0;36mWeightedReplayBuffer.build_memory\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_memory\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m trajs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_dict\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m--> 144\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpush(\n\u001b[1;32m    148\u001b[0m                 state \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[1;32m    149\u001b[0m                 action \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 next_available_actions\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_available_actions\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[1;32m    156\u001b[0m             )\n",
      "File \u001b[0;32m~/Pearl/swarm-rl/custom_buffer.py:28\u001b[0m, in \u001b[0;36mAW\u001b[0;34m(trajs)\u001b[0m\n\u001b[1;32m     25\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(functools\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28;01mlambda\u001b[39;00m a, b: a \u001b[38;5;241m+\u001b[39m b,\n\u001b[1;32m     26\u001b[0m             [[w] \u001b[38;5;241m*\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m w, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((ep_rets \u001b[38;5;241m-\u001b[39m v), ep_lens)]))\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m (weights \u001b[38;5;241m-\u001b[39m weights\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (weights\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m weights\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m---> 28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrajs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrajs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     29\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/Pearl/swarm-rl/custom_buffer.py:28\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(functools\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28;01mlambda\u001b[39;00m a, b: a \u001b[38;5;241m+\u001b[39m b,\n\u001b[1;32m     26\u001b[0m             [[w] \u001b[38;5;241m*\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m w, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m((ep_rets \u001b[38;5;241m-\u001b[39m v), ep_lens)]))\n\u001b[1;32m     27\u001b[0m weights \u001b[38;5;241m=\u001b[39m (weights \u001b[38;5;241m-\u001b[39m weights\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (weights\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m weights\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m---> 28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m {k: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrajs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m trajs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[1;32m     29\u001b[0m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "offline_data_replay_buffer = WeightedReplayBuffer(0.2)\n",
    "print(\"Loading data in buffer...\")\n",
    "for data_path in data_file_paths:\n",
    "    load_transitions(offline_data_replay_buffer, data_path)\n",
    "\n",
    "print(\"Building buffer...\")\n",
    "offline_data_replay_buffer.build_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f355f9856ee816d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:32:03.469406Z",
     "start_time": "2025-01-06T00:32:03.021777Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_learning_logger(\n",
    "    results: dict[str, Any],\n",
    "    step: int,\n",
    "    batch: Optional[TransitionBatch] = None,\n",
    "    prefix: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A learning logger that does nothing.\n",
    "    \"\"\"\n",
    "    if \"loss\" in results:\n",
    "        with open(loss_log_file, \"a\") as f:\n",
    "            f.write(results[\"loss\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324fa51831dfb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:32:03.474924700Z",
     "start_time": "2025-01-03T15:56:58.215893Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Q_value_network = MultiAgentQValueMultiHeadNetwork(\n",
    "    state_dim=OBSERVATION_SPACE,  # dimension of the state representation\n",
    "    action_dim=ACTION_SPACE,                    # dimension of the action representation\n",
    "    hidden_dims=[256,256],                   # dimensions of the intermediate layers\n",
    "    output_dim=ACTION_SPACE,\n",
    "    global_features=3,\n",
    "    local_features=6,\n",
    ")    \n",
    "\n",
    "action_space = DiscreteActionSpace(\n",
    "    actions=list(torch.arange(ACTION_SPACE).view(-1, 1))\n",
    ")\n",
    "\n",
    "agent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=OBSERVATION_SPACE,\n",
    "        action_space=action_space,\n",
    "        batch_size=512,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_value_network, # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=ACTION_SPACE\n",
    "        ),\n",
    "        is_conservative=True\n",
    "    ),\n",
    "    device_id=device_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44aac56bc2d3f1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:32:03.474924700Z",
     "start_time": "2025-01-03T15:56:59.753244Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40349 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40349 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loss_log_file,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43moffline_learning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffline_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffline_data_replay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# replay buffer created using the offline data\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Pearl/pearl/utils/functional_utils/train_and_eval/offline_learning_and_evaluation.py:192\u001b[0m, in \u001b[0;36moffline_learning\u001b[0;34m(offline_agent, data_buffer, training_epochs, number_of_batches, learning_logger, seed)\u001b[0m\n\u001b[1;32m    190\u001b[0m batch \u001b[38;5;241m=\u001b[39m data_buffer\u001b[38;5;241m.\u001b[39msample(offline_agent\u001b[38;5;241m.\u001b[39mpolicy_learner\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, TransitionBatch)\n\u001b[0;32m--> 192\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43moffline_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    194\u001b[0m     learning_logger(loss, i, batch, TRAINING_TAG)\n",
      "File \u001b[0;32m~/Pearl/pearl/pearl_agent.py:231\u001b[0m, in \u001b[0;36mPearlAgent.learn_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03mThis API is often used in offline learning\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03mwhere users pass in a batch of data to train directly\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_learner\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 231\u001b[0m policy_learner_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msafety_module\u001b[38;5;241m.\u001b[39mlearn_batch(batch)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy_learner_loss\n",
      "File \u001b[0;32m~/Pearl/pearl/policy_learners/sequential_decision_making/deep_td_learning.py:317\u001b[0m, in \u001b[0;36mDeepTDLearning.learn_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 317\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Target network update\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pearl-env/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pearl-env/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pearl-env/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "training_epochs = 100\n",
    "experiment_seed = 100\n",
    "\n",
    "with open(loss_log_file,\"w\") as f:\n",
    "    pass\n",
    "\n",
    "offline_learning(\n",
    "    offline_agent=agent,\n",
    "    data_buffer=offline_data_replay_buffer, # replay buffer created using the offline data\n",
    "    training_epochs=training_epochs,\n",
    "    seed=experiment_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4260a2ea2068b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:32:03.476433700Z",
     "start_time": "2025-01-01T18:39:23.591386Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"agent.pkl\", \"wb\") as f:\n",
    "    pickle.dump(agent, f)\n",
    "    \n",
    "\n",
    "torch.save(agent.policy_learner._Q._model, \"Q_function.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8567b923ee978909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:32:03.477439200Z",
     "start_time": "2025-01-01T18:20:30.787783Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't get a batch of size 1 from a replay buffer with only 0 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trans \u001b[38;5;241m=\u001b[39m \u001b[43moffline_data_replay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m agent\u001b[38;5;241m.\u001b[39mreset(trans\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m],action_space)\n\u001b[1;32m      3\u001b[0m agent\u001b[38;5;241m.\u001b[39mact(exploit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Pearl/swarm-rl/custom_buffer.py:116\u001b[0m, in \u001b[0;36mWeightedReplayBuffer.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mThe shapes of input and output are:\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03minput: batch_size\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get a batch of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from a replay buffer with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m samples \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weights, k\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_transition_batch(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# pyre-fixme[6]: For 1st argument expected `List[Transition]` but got\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m#  `List[Union[Transition, TransitionBatch]]`.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     transitions\u001b[38;5;241m=\u001b[39msamples,\n\u001b[1;32m    125\u001b[0m     is_action_continuous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_action_continuous,\n\u001b[1;32m    126\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Can't get a batch of size 1 from a replay buffer with only 0 elements"
     ]
    }
   ],
   "source": [
    "trans = offline_data_replay_buffer.sample(1)\n",
    "agent.reset(trans.state[0],action_space)\n",
    "agent.act(exploit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f016041d6aba50c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T00:32:03.478440100Z",
     "start_time": "2025-01-01T18:37:37.582388Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mpolicy_learner\u001b[38;5;241m.\u001b[39m_Q\u001b[38;5;241m.\u001b[39m_model\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:1187\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:627\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:937\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:928\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython_win32_311_64.pyx:585\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional 2024.2.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1207\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001b[0m\n\u001b[0;32m   1204\u001b[0m         from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_id)\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 1207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional 2024.2.1\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1222\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001b[0m\n\u001b[0;32m   1219\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_mpl_hook()\n\u001b[0;32m   1221\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 1222\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.policy_learner._Q._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f215c57ed4c2d4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pearl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
