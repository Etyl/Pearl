{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:07:14.618859Z",
     "start_time": "2025-01-06T20:07:10.683580Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import math\n",
    "from typing import Optional, Any\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from custom_buffer import WeightedReplayBuffer\n",
    "\n",
    "from pearl.replay_buffers import BasicReplayBuffer\n",
    "from pearl.utils.instantiations.spaces import DiscreteActionSpace\n",
    "from pearl import PearlAgent\n",
    "from pearl.utils.functional_utils.train_and_eval.learning_logger import LearningLogger, null_learning_logger\n",
    "from pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import TRAINING_TAG\n",
    "from pearl.replay_buffers import TransitionBatch, ReplayBuffer\n",
    "from pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from pearl.neural_networks.sequential_decision_making import VanillaContinuousActorNetwork\n",
    "from pearl.policy_learners.sequential_decision_making import ImplicitQLearning\n",
    "from pearl.utils.functional_utils.train_and_eval.offline_learning_and_evaluation import offline_evaluation,offline_learning\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork, VanillaQValueMultiHeadNetwork\n",
    "from pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "from pearl.neural_networks.sequential_decision_making.q_value_networks import MultiAgentQValueMultiHeadNetwork\n",
    "\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5932d4a9f11e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:07:14.628361Z",
     "start_time": "2025-01-06T20:07:14.623701Z"
    }
   },
   "outputs": [],
   "source": [
    "FRONTIER_COUNT:int = 6 # the maximum number of selected frontiers\n",
    "FRONTIER_FEATURES:int = 6 # the number of features measured for each frontier\n",
    "OTHER_FRONTIER_INPUTS:int = 3 # other observations (current % explored area)\n",
    "\n",
    "OBSERVATION_SPACE:int = FRONTIER_COUNT*FRONTIER_FEATURES+OTHER_FRONTIER_INPUTS\n",
    "ACTION_SPACE:int = FRONTIER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3216b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:07:15.372617Z",
     "start_time": "2025-01-06T20:07:15.366932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 3\n",
      "GPU 0: Tesla V100-PCIE-16GB\n",
      "GPU 1: Tesla V100-PCIE-32GB\n",
      "GPU 2: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPUs available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3937b0139a19940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:07:15.392544Z",
     "start_time": "2025-01-06T20:07:15.387411Z"
    }
   },
   "outputs": [],
   "source": [
    "replay_buffer_size = 1_000_000\n",
    "device_id = 0\n",
    "is_action_continuous = False\n",
    "data_file_paths = [\n",
    "    \"data/rl-run-data.json\",\n",
    "    \"data/rl-run-dqn-1-data.json\",\n",
    "    \"data/rl-run-dqn-13-data.json\",\n",
    "    # \"data/rl-run-dqn-7-data.json\"\n",
    "]\n",
    "# data_file_paths = [\"data/rl-run-data.json\"]\n",
    "max_number_actions_if_discrete = ACTION_SPACE\n",
    "loss_log_file = \"loss.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801dfefd86e8147e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:07:15.424834Z",
     "start_time": "2025-01-06T20:07:15.413748Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_transitions(buffer, path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data_transitions = json.load(f)\n",
    "    \n",
    "    count = 0\n",
    "    for map_name in data_transitions:\n",
    "        for i in range(len(data_transitions[map_name])):\n",
    "            transitions = data_transitions[map_name][i][\"run\"]\n",
    "            episode = defaultdict(list)\n",
    "            score = np.sum(data_transitions[map_name][i][\"score\"])\n",
    "            for j in range(len(transitions[\"obs\"])):\n",
    "                transition = {}\n",
    "\n",
    "                transition[\"score\"] = [score]\n",
    "                \n",
    "                transition[\"observation\"] = np.array(transitions[\"obs\"][j], dtype=np.float32)\n",
    "                transition[\"action\"] = np.argmax(transitions[\"action\"][j])\n",
    "                transition[\"next_observation\"] = np.array(transitions[\"next_obs\"][j], dtype=np.float32)\n",
    "                transition[\"reward\"] = np.float32(\n",
    "                    transitions[\"reward\"][j][0] +\n",
    "                    10*transitions[\"obs\"][j][37]*transitions[\"obs\"][j][38]\n",
    "                )\n",
    "                transition[\"curr_available_actions\"] = DiscreteActionSpace(\n",
    "                    actions=list(\n",
    "                        torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                    )\n",
    "                )\n",
    "                transition[\"next_available_actions\"] = DiscreteActionSpace(\n",
    "                    actions=list(\n",
    "                        torch.arange(len(transitions[\"action\"][j])).view(-1, 1)\n",
    "                    )\n",
    "                )\n",
    "                transition[\"terminated\"] = False\n",
    "                transition[\"truncated\"] = False\n",
    "                \n",
    "                # Ignore terminated transitions\n",
    "                if transition[\"reward\"] > 0.995:\n",
    "                    continue\n",
    "                    \n",
    "                # Suffle frontier order\n",
    "                permutation = np.random.permutation(FRONTIER_COUNT)\n",
    "                \n",
    "                for obs in [\"observation\", \"next_observation\"]:\n",
    "                    for i in range(FRONTIER_FEATURES):               \n",
    "                        transition[obs][FRONTIER_COUNT*i:FRONTIER_COUNT*(i+1)] = transition[obs][FRONTIER_COUNT*i:FRONTIER_COUNT*(i+1)][permutation]\n",
    "                \n",
    "                transition[\"action\"] = permutation[transition[\"action\"]]\n",
    "\n",
    "                assert len(transitions[\"action\"][j]) == ACTION_SPACE\n",
    "                assert len(transition[\"observation\"]) == OBSERVATION_SPACE\n",
    "                assert len(transition[\"next_observation\"]) == OBSERVATION_SPACE\n",
    "                \n",
    "                count += 1\n",
    "                for key in transition:\n",
    "                    episode[key].append(transition[key])\n",
    "                    \n",
    "            buffer.add_traj(episode, map_name)\n",
    "    print(f\"Loaded {count} transitions from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b8eba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:08:42.183935Z",
     "start_time": "2025-01-06T20:07:15.439306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in buffer...\n",
      "Loaded 206585 transitions from data/rl-run-data.json\n",
      "Loaded 327293 transitions from data/rl-run-dqn-1-data.json\n",
      "Loaded 204648 transitions from data/rl-run-dqn-13-data.json\n",
      "Building buffer...\n"
     ]
    }
   ],
   "source": [
    "offline_data_replay_buffer = WeightedReplayBuffer(1)\n",
    "print(\"Loading data in buffer...\")\n",
    "for data_path in data_file_paths:\n",
    "    load_transitions(offline_data_replay_buffer, data_path)\n",
    "\n",
    "print(\"Building buffer...\")\n",
    "offline_data_replay_buffer.build_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f355f9856ee816d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:08:42.208465Z",
     "start_time": "2025-01-06T20:08:42.195466Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_learning_logger(\n",
    "    results: dict[str, Any],\n",
    "    step: int,\n",
    "    batch: Optional[TransitionBatch] = None,\n",
    "    prefix: str = \"\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    A learning logger that does nothing.\n",
    "    \"\"\"\n",
    "    if \"loss\" in results:\n",
    "        with open(loss_log_file, \"a\") as f:\n",
    "            f.write(f\"{results['loss']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324fa51831dfb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:08:43.745050Z",
     "start_time": "2025-01-06T20:08:42.233663Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Q_value_network = MultiAgentQValueMultiHeadNetwork(\n",
    "    state_dim=OBSERVATION_SPACE,  # dimension of the state representation\n",
    "    action_dim=ACTION_SPACE,                    # dimension of the action representation\n",
    "    hidden_dims=[],                 # dimensions of the intermediate layers\n",
    "    output_dim=ACTION_SPACE,\n",
    "    global_features=3,\n",
    "    local_features=6,\n",
    ")    \n",
    "\n",
    "action_space = DiscreteActionSpace(\n",
    "    actions=list(torch.arange(ACTION_SPACE).view(-1, 1))\n",
    ")\n",
    "\n",
    "agent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=OBSERVATION_SPACE,\n",
    "        action_space=action_space,\n",
    "        learning_rate = 0.0001,\n",
    "        batch_size=512,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_value_network, # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=ACTION_SPACE\n",
    "        ),\n",
    "        is_conservative=False\n",
    "    ),\n",
    "    device_id=device_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c44aac56bc2d3f1d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-06T20:08:43.755426Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43274 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43274/43274 [37:57<00:00, 19.00it/s] \n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "training_epochs = 30\n",
    "experiment_seed = 100\n",
    "\n",
    "with open(loss_log_file,\"w\") as f:\n",
    "    pass\n",
    "\n",
    "offline_learning(\n",
    "    offline_agent=agent,\n",
    "    data_buffer=offline_data_replay_buffer, # replay buffer created using the offline data\n",
    "    training_epochs=training_epochs,\n",
    "    seed=experiment_seed,\n",
    "    learning_logger=loss_learning_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4260a2ea2068b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T20:22:31.182985Z",
     "start_time": "2025-01-01T18:39:23.591386Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"agent.pkl\", \"wb\") as f:\n",
    "    pickle.dump(agent, f)\n",
    "    \n",
    "\n",
    "torch.save(agent.policy_learner._Q._model, \"Q_function.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pearl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
